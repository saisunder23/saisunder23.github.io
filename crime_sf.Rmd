---
title: "Analyzing Crime in San Francisco"
author: "Sunder Sai"
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, fig.align="center", echo=FALSE}
library(jpeg)
library(grid)
map1<- readJPEG("images/sf_map.JPG")
grid.raster(map1, just = "center")
```

## **San Francisco Crime Patterns**

In this R-markdown project, I leverage publicly available data to interpret crime patterns within the city of San Francisco. I utilize my skills in R through data manipulation with dplyr package as well as data visualization with ggplot 2. This project was originally completed through [DataCamp](https://learn.datacamp.com/projects/614) using a Jupyter notebook. All codes are annotated.  

  
  
***
## Step 1: Loading Packages & Data

*  In this first code chunk, I utilize the `tidyverse` and `lubridate` packages to read the CSV files from the city of San Francisco website. 

```{r Reading Data, message=FALSE, warning=FALSE, results=FALSE}

#Load libraries
library(lubridate)
library(tidyverse)

# Read in incidents dataset
incidents <- read_csv("data/downsample_police-department-incidents.csv")

# Read in calls dataset
calls <- read_csv("data/downsample_police-department-calls-for-service.csv")

```

## Step 2: Inspecting data and generate frequency statistic

*  In this next code chunk, I `glimpse` the incidents and calls datasets to understand the structure of the data.   
*  I then count the number of reported incidents by `Date` and rename the column of counts to `n_incidents`.  
*  I then count the number of civilian phone calls for police service by Date and rename the columns of counts to `n_calls`, assigning the output to `daily_calls`.


```{r inspection, message=FALSE, warning=FALSE, results=FALSE}

# Glimpse structure of both datasets
glimpse(incidents)
glimpse(calls)

# Aggregate the number of reported incidents by Date
daily_incidents <- incidents %>%
    count(Date, sort = TRUE) %>%
    rename(n_incidents = n)

# Aggregate the number of calls for police service by Date
daily_calls <- calls %>%
    count(Date, sort = TRUE) %>%
   rename(n_calls = n)

```

## Step 3: Using Mutating Joins to join the datasets by date

Here, I use `inner_join()` to join `daily_calls` to `daily_incidents`, assigning the output to a variable named `shared_dates`. I then inspect the new dataframe. 


```{r join}
#inner join
shared_dates<-inner_join(daily_calls, daily_incidents, by="Date")
glimpse(shared_dates)
```


## Step 4: Inspect Frequency

*  In this R-chunk, I reshape the data and visualize the trends for incidents and calls on the same graph. I then create a "long format" of the data frame (called plot_shared_dates) using the `gather()` function.  
*  I then use `ggplot()` in order to visualize `Date` vs. `Count`, and color by `report`. I overlay a linear model to visualize the trends in the data. 

``` {r frequency }
# Gather into long format using the "Date" column to define observations
plot_shared_dates <- shared_dates %>%
  gather(key = report, value = count, -Date)

# Plot points and regression trend lines
ggplot(plot_shared_dates, aes(x = Date, y = count, color = report)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x)

```


## Step 5: Evaluate correlation between trends 

*  In this code chunk, I calculate the correlation coefficient between the frequency of incidents and calls. This is a way we can discern the relationship between 2 variables. When the correlation coefficient is -1 there is no correlation, and when it is +1 there is perfect correlation.   
To do this I create a new column, `month`, from the `Date` column of the previous dataset. I then `group_by()` this new column in order to `summarize()` the new frequency counts. I then use the `cor()` function to calculate the correlation. 

``` {r trends}
# Calculate correlation coefficient between daily frequencies
daily_cor <- cor(shared_dates$n_incidents, shared_dates$n_calls)
daily_cor

# Summarize frequencies by month
correlation_df <- shared_dates %>% 
  mutate(month = month(Date)) %>%
  group_by(month) %>% 
  summarize(n_incidents = sum(n_incidents),
            n_calls = sum(n_calls))

# Calculate correlation coefficient between monthly frequencies
monthly_cor <- cor(correlation_df$n_incidents, correlation_df$n_calls)
monthly_cor


```

We see that there is a correlation coefficient of 0.97 which indicates a high correlation between daily frequencies.  


## Step 6: Filtering joins to better clean/understand data

* It may be helpful to subset information based on antoher set of values. In this code chunk I use `filtering joins` to keep the information from each police reported incident and each civilian call on their shared dates in order to calculate similar statistics and compare results.   

``` {r filtering}

# Subset calls to police by shared_dates
calls_shared_dates <- semi_join(calls, incidents, by= c("Date"="Date"))

# Perform a check that we are using this filtering join function appropriately
identical(sort(unique(shared_dates$Date)), sort(unique(calls_shared_dates$Date)))

# Filter recorded incidents by shared_dates
incidents_shared_dates <- semi_join(incidents, calls, by= c("Date", "Date"))
```

## Step 7: Ranking the top call and incident crime types

* Here, in this code chunk I rank the top call and incident crime types by frequency and visualize with a histogram.  
I subset the top 15 crime types in descending order by count for both calls and incidents and use the pipe to pass the information to `ggplot()`. 

``` {r crime}

# Creating bar chart of the number of calls for each crime
plot_calls_freq <- calls_shared_dates %>% 
  count(Descript) %>% 
  top_n(15, n) %>% 
  ggplot(aes(x = reorder(Descript, n), y = n)) +
  geom_bar(stat = 'identity') +
  ylab("Count") +
  xlab("Crime Description") +
  ggtitle("Calls Reported Crimes") +
  coord_flip()
  

# Creating bar chart of the number of reported incidents for each crime
plot_incidents_freq <- incidents_shared_dates %>% 
  count(Descript) %>% 
  top_n(15, n)  %>% 
  ggplot(aes(x = reorder(Descript, n), y = n)) +
  geom_bar(stat = 'identity') +
  ylab("Count") +
  xlab("Crime Description") +
  ggtitle("Incidents Reported Crimes") +
  coord_flip()

# Outputting the plots
plot_calls_freq
plot_incidents_freq
```

## Step 8: Checking to see if there are similar locations of crimes

We can observe that the crime of highest incidence is Grand Theft From Locked Auto. The 12th most civilian reported crime is "Auto Boost/Strip." There may be a possibility that the location of the called in crime is similar to the location of the crime incidence. We should check to see if the locations of the most frequent civilian reproted crime and police reported crime are similar. 

``` {r auto, message=FALSE, warning=FALSE, results=FALSE}
# Arranging the top 10 locations of called in crimes in a new variable
location_calls <- calls_shared_dates %>%
  filter(Descript == "Auto Boost / Strip") %>% 
  count(`Address`) %>% 
  arrange(desc(n))%>% 
  top_n(10, n)

# Arranging the top 10 locations of reported incidents in a new variable
location_incidents <- incidents_shared_dates %>%
  filter(Descript == "GRAND THEFT FROM LOCKED AUTO") %>% 
  count(`Address`) %>% 
  arrange(desc(n))%>% 
  top_n(10, n)

# Printing the top locations of each dataset for comparison
location_calls
location_incidents

```


## Step 9: San Francisco density plot 

Here, I visualize a 2D density plot on a map of San Francisco.   
I use the `ggmap` package and read a preprocessed map of San Francisco (sf_map)  
I `filter()` by grand theft auto and save this into a new dataframe. I then overlay the lattitude and longitude data using `stat_density_2d()`

``` {r mapping, eval=FALSE}

# Load ggmap

library(ggmap)
# Read in a static map of San Francisco 
sf_map <- readRDS("/datasets/sf_map.RDS")

# Filter grand theft auto incidents
auto_incidents <- incidents_shared_dates %>% 
    filter(Descript == "GRAND THEFT FROM LOCKED AUTO")

# Overlay a density plot of auto incidents on the map
ggmap(sf_map) +
  stat_density_2d(
    aes(x = X, y = Y, fill = ..level..), alpha = 0.15,
    size = 0.01, bins = 30, data = auto_incidents,
    geom = "polygon")

```

```{r, fig.align="center", echo=FALSE}
library(jpeg)
library(grid)
map1<- readJPEG("images/sf_map.JPG")
grid.raster(map1, just = "center")
```